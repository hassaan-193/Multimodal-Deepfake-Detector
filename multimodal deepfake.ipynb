{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchaudio\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
        "\n",
        "import moviepy.editor as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "!pip install json5\n",
        "import json5\n",
        "import sys\n",
        "\n",
        "!pip install pytube\n",
        "from pytube import YouTube\n",
        "!pip install ffmpeg-python\n",
        "import ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siSnh1ocX-HI",
        "outputId": "81e03adf-b279-4f1c-ddd6-91c3a368e494"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting json5\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5\n",
            "Successfully installed json5-0.12.1\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folders = [\n",
        "    \"Data/videos\",\n",
        "    \"Data/audio\",\n",
        "    \"Data/frames\",\n",
        "    \"Data/spectrograms\",\n",
        "    \"Models/video_model\",\n",
        "    \"Models/audio_model\",\n",
        "    \"Models/multimodal\",\n",
        "    \"Data/text_features\"\n",
        "]\n",
        "\n",
        "for f in folders:\n",
        "  os.makedirs(f, exist_ok=True)\n",
        "\n",
        "print(\"Folder structure created successfully\")"
      ],
      "metadata": {
        "id": "31lf_tf7khXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5827bebf-77f5-4ae0-c370-4b5087a55002"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder structure created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "input_folder = \"Data/videos\"\n",
        "os.makedirs(input_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    with open(os.path.join(input_folder, filename), \"wb\") as f:\n",
        "        f.write(uploaded[filename])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "FrvYBSagD38x",
        "outputId": "8c94dc44-b44a-499f-fad0-bd3f35725598"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c69632eb-127c-4734-9e24-32911030cc88\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c69632eb-127c-4734-9e24-32911030cc88\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WIN_20251223_22_25_20_Pro.mp4 to WIN_20251223_22_25_20_Pro.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract frames from videos & images\n",
        "\n",
        "input_folder = \"Data/videos\"\n",
        "output_folder = \"Data/frames\"\n",
        "\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "FRAME_RATE = 1\n",
        "\n",
        "\n",
        "def extract_frames_from_video(video_path, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    frame_interval = int(fps / FRAME_RATE) if fps > 0 else 1\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "\n",
        "        if frame_count % frame_interval == 0:\n",
        "            frame_path = os.path.join(save_dir, f\"frame_{saved_count:04d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "def copy_image(img_path, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    img = Image.open(img_path)\n",
        "    if img.mode == 'RGBA':\n",
        "        img = img.convert('RGB')\n",
        "    img.save(os.path.join(save_dir, \"image_0001.jpg\"))\n",
        "\n",
        "\n",
        "for file_name in tqdm(os.listdir(input_folder), desc=\"Processing files\"):\n",
        "    file_path = os.path.join(input_folder, file_name)\n",
        "    save_directory = os.path.join(output_folder, os.path.splitext(file_name)[0])\n",
        "\n",
        "    if file_name.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\", \".flv\")):\n",
        "        print(f\"\\nExtracting frames from video: {file_name}\")\n",
        "        extract_frames_from_video(file_path, save_directory)\n",
        "\n",
        "    elif file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "        print(f\"\\nCopying image: {file_name}\")\n",
        "        copy_image(file_path, save_directory)\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nSkipping Unsupported Files: {file_name}\")\n",
        "\n",
        "print(\"\\nFrame extraction successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spCCs0l45Vb3",
        "outputId": "bfb30991-0ba7-4bd5-9b68-5dcc617f5768"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skipping Unsupported Files: DDD_samples.gif\n",
            "\n",
            "Extracting frames from video: video_file.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  10%|█         | 2/20 [00:04<00:37,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skipping Unsupported Files: deepfakes.gif\n",
            "\n",
            "Copying image: ex_deepfakes.png\n",
            "\n",
            "Copying image: ex_original.png\n",
            "\n",
            "Skipping Unsupported Files: neuraltextures.gif\n",
            "\n",
            "Extracting frames from video: WIN_20251223_22_25_20_Pro.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 20/20 [00:09<00:00,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skipping Unsupported Files: faceshifter.gif\n",
            "\n",
            "Skipping Unsupported Files: face2face.gif\n",
            "\n",
            "Copying image: ex_deepfakes_mask.png\n",
            "\n",
            "Copying image: ex_deepfakedetection.png\n",
            "\n",
            "Skipping Unsupported Files: deepfakedetection.gif\n",
            "\n",
            "Copying image: table4_faceshifter.png\n",
            "\n",
            "Skipping Unsupported Files: faceswap.gif\n",
            "\n",
            "Copying image: ex_neuraltextures_mask.png\n",
            "\n",
            "Copying image: ex_original_actors.png\n",
            "\n",
            "Copying image: teaser.png\n",
            "\n",
            "Copying image: ex_neuraltextures.png\n",
            "\n",
            "Copying image: ex_deepfakedetection_mask.png\n",
            "\n",
            "Skipping Unsupported Files: .ipynb_checkpoints\n",
            "\n",
            "Frame extraction successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from librosa.feature import chroma_stft\n",
        "# Extract AUdio and Audio Features\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "video_folder = \"Data/videos\"\n",
        "audio_folder = \"Data/audio\"\n",
        "feature_folder = \"Data/audio_features\"\n",
        "\n",
        "os.makedirs(audio_folder, exist_ok = True)\n",
        "os.makedirs(feature_folder, exist_ok = True)\n",
        "\n",
        "def extract_audio(video_path, save_path):\n",
        "  video = VideoFileClip(video_path)\n",
        "  audio = video.audio\n",
        "  audio.write_audiofile(save_path, fps =  16000)\n",
        "  video.close()\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "  y, sr = librosa.load(audio_path, sr = 16000)\n",
        "\n",
        "  features = {\n",
        "      \"mfcc\" : librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
        "      \"mel\"  : librosa.feature.melspectrogram(y=y, sr=sr),\n",
        "      \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "  }\n",
        "  return features\n",
        "\n",
        "for file in tqdm(os.listdir(video_folder), desc = \"Processing Videos\"):\n",
        "  if file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\", \".flv\")):\n",
        "    video_path = os.path.join(video_folder, file)\n",
        "    base_name = os.path.splitext(file)[0]\n",
        "    audio_output_path = os.path.join(audio_folder, base_name + \".wav\")\n",
        "\n",
        "\n",
        "    print(f\"\\n Extracting audio from: {file}\")\n",
        "    extract_audio(video_path, audio_output_path)\n",
        "\n",
        "    print(\"Extracting audio features..\")\n",
        "    features = extract_audio_features(audio_output_path)\n",
        "\n",
        "\n",
        "    feature_save_path = os.path.join(\n",
        "        feature_folder, base_name + \".npy\"\n",
        "    )\n",
        "    np.save(feature_save_path, features)\n",
        "\n",
        "print(\"\\n Audio and Audio Features extracted succesfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W7SXg0K72VP",
        "outputId": "32cc15c4-67d2-4d7f-9180-d81d84c8593e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Videos:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Extracting audio from: video_file.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Videos:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in Data/audio/video_file.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "chunk:   0%|          | 0/323 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "chunk:  35%|███▌      | 114/323 [00:00<00:00, 1096.03it/s, now=None]\u001b[A\n",
            "chunk:  69%|██████▉   | 224/323 [00:00<00:00, 1046.54it/s, now=None]\u001b[A\n",
            "Processing Videos:   0%|          | 0/20 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Extracting audio features..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n",
            "\n",
            "Processing Videos:  10%|█         | 2/20 [00:02<00:26,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Extracting audio from: WIN_20251223_22_25_20_Pro.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Videos:  10%|█         | 2/20 [00:03<00:26,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in Data/audio/WIN_20251223_22_25_20_Pro.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "chunk:   0%|          | 0/551 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "chunk:  34%|███▍      | 187/551 [00:00<00:00, 1868.10it/s, now=None]\u001b[A\n",
            "chunk:  68%|██████▊   | 374/551 [00:00<00:00, 1856.47it/s, now=None]\u001b[A\n",
            "Processing Videos:  10%|█         | 2/20 [00:03<00:26,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Extracting audio features..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Videos: 100%|██████████| 20/20 [00:04<00:00,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Audio and Audio Features extracted succesfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Speech to text + tetx feature extraction\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install transformers sentencepiece\n",
        "\n",
        "import whisper\n",
        "\n",
        "\n",
        "audio_folder = \"Data/audio\"\n",
        "text_folder  = \"Data/text\"\n",
        "text_features_folder = \"Data/text_features\"\n",
        "\n",
        "\n",
        "os.makedirs(text_folder, exist_ok = True)\n",
        "os.makedirs(text_features_folder, exist_ok = True)\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embed_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_sentence_embedding(text):\n",
        "  tokens = embed_tokenizer(text, return_tensors = \"pt\", truncation = True, padding = True)\n",
        "  with torch.no_grad():\n",
        "    output = embed_model(**tokens)\n",
        "  return output.last_hidden_state.mean(dim = 1).numpy()\n",
        "\n",
        "for file in tqdm(os.listdir(audio_folder), desc = \"Speech_To_Text\"):\n",
        "  if file.lower().endswith(\".wav\"):\n",
        "\n",
        "    audio_path = os.path.join (audio_folder, file)\n",
        "    text_output_path = os.path.join(text_folder, file.replace(\".wav\", \".txt\"))\n",
        "    feature_output = os.path.join(text_features_folder, file.replace(\".wav\", \".npy\" ))\n",
        "\n",
        "    print(f\"\\n Transcribing: {file}\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    text = result[\"text\"]\n",
        "\n",
        "    with open(text_output_path, \"w\") as f:\n",
        "      f.write(text)\n",
        "\n",
        "    print(\" Extracting text features...\")\n",
        "    embedding = get_sentence_embedding(text)\n",
        "\n",
        "    np.save(feature_output, embedding)\n",
        "\n",
        "print(\"\\n Speech_To_Text + Text_ Features Exttracted\")"
      ],
      "metadata": {
        "id": "_MlB-Wlbkrim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c424b338-46de-4b1d-8892-3bd350a67518"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-uw5z6zua\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-uw5z6zua\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.5.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSpeech_To_Text:   0%|          | 0/2 [00:00<?, ?it/s]WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Transcribing: WIN_20251223_22_25_20_Pro.wav\n",
            " Extracting text features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSpeech_To_Text:  50%|█████     | 1/2 [00:15<00:15, 15.27s/it]WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Transcribing: video_file.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speech_To_Text: 100%|██████████| 2/2 [00:20<00:00, 10.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Extracting text features...\n",
            "\n",
            " Speech_To_Text + Text_ Features Exttracted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import cv2  # OpenCV for image processing\n",
        "import dlib  # For face detection and alignment\n",
        "import os   # For handling file paths\n",
        "\n",
        "# Check if the dlib shape predictor file exists, if not, download it\n",
        "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
        "if not os.path.exists(predictor_path):\n",
        "    print(f\"Downloading {predictor_path}...\")\n",
        "    # This URL is for the 68-point face landmark detector from dlib's github\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2 # Decompress\n",
        "\n",
        "# Initialize face detector and shape predictor\n",
        "detector = dlib.get_frontal_face_detector()  # Detects faces in an image\n",
        "predictor = dlib.shape_predictor(predictor_path)\n",
        "# Pre-trained model to locate 68 facial landmarks for alignment\n",
        "\n",
        "# Function to crop and align face from an image\n",
        "def crop_align_face(image_path, output_path):\n",
        "    # Read image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert image to grayscale (dlib works better on grayscale)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the image\n",
        "    faces = detector(gray)\n",
        "\n",
        "    # Loop through detected faces\n",
        "    for i, face in enumerate(faces):\n",
        "        # Get landmarks for the face\n",
        "        landmarks = predictor(gray, face)\n",
        "\n",
        "        # Extract the coordinates for eyes (for alignment)\n",
        "        left_eye = (landmarks.part(36).x, landmarks.part(36).y)\n",
        "        right_eye = (landmarks.part(45).x, landmarks.part(45).y)\n",
        "\n",
        "        # Calculate angle between eyes\n",
        "        dy = right_eye[1] - left_eye[1]\n",
        "        dx = right_eye[0] - left_eye[0]\n",
        "        angle = cv2.fastAtan2(dy, dx)\n",
        "\n",
        "        # Compute center of eyes\n",
        "        eyes_center = ((left_eye[0] + right_eye[0]) // 2,\n",
        "                       (left_eye[1] + right_eye[1]) // 2)\n",
        "\n",
        "        # Get rotation matrix for alignment\n",
        "        M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
        "\n",
        "        # Rotate the image to align eyes horizontally\n",
        "        aligned_face = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
        "\n",
        "        # Crop the face using the original face rectangle\n",
        "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "        cropped_face = aligned_face[y:y+h, x:x+w]\n",
        "\n",
        "        # Save cropped and aligned face\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        cv2.imwrite(os.path.join(output_path, f'face_{i}.jpg'), cropped_face)"
      ],
      "metadata": {
        "id": "tdj_rtV96mW1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUdio MOdel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# The AudioModel class definition has been moved to cell W-lU2qGHJYXT\n",
        "# This cell is now empty or can be removed if not needed for other purposes."
      ],
      "metadata": {
        "id": "CQO-NYthtQkp"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# The TextModel class definition has been moved to cell FADbM2wULsvC\n",
        "# This cell is now empty or can be removed if not needed for other purposes."
      ],
      "metadata": {
        "id": "g2rBEVrbIItr"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# The VisionModel class definition has been moved to cell dP-cJzC4NBN6\n",
        "# This cell is now empty or can be removed if not needed for other purposes."
      ],
      "metadata": {
        "id": "npp2PRGVJQHx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multimodal Fusion\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultimodalFusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultimodalFusion, self).__init__()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_emb, text_emb, vision_emb):\n",
        "        fused = torch.cat((audio_emb, text_emb, vision_emb), dim=1)\n",
        "        return self.classifier(fused)\n"
      ],
      "metadata": {
        "id": "1lUJQMhtNqWX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Audio Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Audio Model Definition (moved from cell CQO-NYthtQkp)\n",
        "class AudioModel(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2):\n",
        "        super(AudioModel, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
        "        return self.fc(h)\n",
        "\n",
        "# ------------------ DEVICE ------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------ MODEL + CLASSIFIER ------------------\n",
        "audio_model = AudioModel().to(device)\n",
        "audio_classifier = nn.Sequential(\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 1)\n",
        ").to(device)\n",
        "\n",
        "# ------------------ OPTIMIZER & LOSS ------------------\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(audio_model.parameters()) + list(audio_classifier.parameters()), lr=2e-4\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# ------------------ TRAIN LOOP ------------------\n",
        "def train_one_epoch(dataloader):\n",
        "    audio_model.train()\n",
        "    audio_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        audio_x = batch[\"audio\"].to(device)\n",
        "        labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "        audio_emb = audio_model(audio_x)\n",
        "        outputs = audio_classifier(audio_emb).squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# ------------------ SAVE MODEL ------------------\n",
        "def save_model():\n",
        "    torch.save(audio_model.state_dict(), \"audio_model.pt\")\n",
        "    torch.save(audio_classifier.state_dict(), \"audio_classifier.pt\")\n",
        "\n",
        "# Call save_model to create the .pt files\n",
        "save_model()"
      ],
      "metadata": {
        "id": "W-lU2qGHJYXT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Text Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# TextModel Definition (moved from cell g2rBEVrbIItr)\n",
        "from transformers import BertModel\n",
        "\n",
        "class TextModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextModel, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.fc = nn.Linear(768, 256)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.fc(cls_embedding)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text_model = TextModel().to(device)\n",
        "text_classifier = nn.Sequential(\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 1)\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(text_model.parameters()) + list(text_classifier.parameters()), lr=2e-4\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def train_one_epoch(dataloader):\n",
        "    text_model.train()\n",
        "    text_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "        text_emb = text_model(input_ids, attention_mask)\n",
        "        outputs = text_classifier(text_emb).squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def save_model():\n",
        "    torch.save(text_model.state_dict(), \"text_model.pt\")\n",
        "    torch.save(text_classifier.state_dict(), \"text_classifier.pt\")\n",
        "\n",
        "# Call save_model to create the .pt files\n",
        "save_model()"
      ],
      "metadata": {
        "id": "FADbM2wULsvC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train Vision MOdel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "# from vision_model import VisionModel # This import is no longer needed\n",
        "\n",
        "# VisionModel Definition (moved from cell npp2PRGVJQHx)\n",
        "from torchvision import models\n",
        "\n",
        "class VisionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisionModel, self).__init__()\n",
        "\n",
        "        self.cnn = models.efficientnet_b0(pretrained=True)\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "        self.fc = nn.Linear(1280, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.cnn(x)\n",
        "        return self.fc(features)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vision_model = VisionModel().to(device)\n",
        "vision_classifier = nn.Sequential(\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 1)\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(vision_model.parameters()) + list(vision_classifier.parameters()), lr=2e-4\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def train_one_epoch(dataloader):\n",
        "    vision_model.train()\n",
        "    vision_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        images = batch[\"images\"].to(device)\n",
        "        labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "        vision_emb = vision_model(images)\n",
        "        outputs = vision_classifier(vision_emb).squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def save_model():\n",
        "    torch.save(vision_model.state_dict(), \"vision_model.pt\")\n",
        "    torch.save(vision_classifier.state_dict(), \"vision_classifier.pt\")\n",
        "\n",
        "# Call save_model to create the .pt files\n",
        "save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP-cJzC4NBN6",
        "outputId": "638a7f07-4a2d-45c9-d197-1737137880c5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FUsion MOdel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Audio Model Definition (copied from W-lU2qGHJYXT)\n",
        "class AudioModel(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2):\n",
        "        super(AudioModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
        "        return self.fc(h)\n",
        "\n",
        "# TextModel Definition (copied from FADbM2wULsvC)\n",
        "from transformers import BertModel\n",
        "\n",
        "class TextModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.fc = nn.Linear(768, 256)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.fc(cls_embedding)\n",
        "\n",
        "# VisionModel Definition (copied from dP-cJzC4NBN6)\n",
        "from torchvision import models\n",
        "\n",
        "class VisionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisionModel, self).__init__()\n",
        "        self.cnn = models.efficientnet_b0(pretrained=True)\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "        self.fc = nn.Linear(1280, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.cnn(x)\n",
        "        return self.fc(features)\n",
        "\n",
        "# MultimodalFusion Definition (copied from 1lUJQMhtNqWX)\n",
        "class MultimodalFusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultimodalFusion, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_emb, text_emb, vision_emb):\n",
        "        fused = torch.cat((audio_emb, text_emb, vision_emb), dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# ------------------ DEVICE ------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------ LOAD PRETRAINED MODELS ------------------\n",
        "audio_model = AudioModel().to(device)\n",
        "text_model = TextModel().to(device)\n",
        "vision_model = VisionModel().to(device)\n",
        "\n",
        "audio_model.load_state_dict(torch.load(\"audio_model.pt\"))\n",
        "text_model.load_state_dict(torch.load(\"text_model.pt\"))\n",
        "vision_model.load_state_dict(torch.load(\"vision_model.pt\"))\n",
        "\n",
        "# Freeze pretrained models\n",
        "for model in [audio_model, text_model, vision_model]:\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# ------------------ FUSION NETWORK ------------------\n",
        "fusion_model = MultimodalFusion().to(device)\n",
        "\n",
        "# ------------------ OPTIMIZER & LOSS ------------------\n",
        "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=2e-4)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# ------------------ TRAIN LOOP ------------------\n",
        "def train_one_epoch(dataloader):\n",
        "    fusion_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Load inputs\n",
        "        audio_x = batch[\"audio\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        images = batch[\"images\"].to(device)\n",
        "        labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "        # Get embeddings from frozen models\n",
        "        audio_emb = audio_model(audio_x)\n",
        "        text_emb = text_model(input_ids, attention_mask)\n",
        "        vision_emb = vision_model(images)\n",
        "\n",
        "        # Fusion network forward\n",
        "        outputs = fusion_model(audio_emb, text_emb, vision_emb).squeeze(1)\n",
        "\n",
        "        # Compute loss and update\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# ------------------ VALIDATION ------------------\n",
        "def validate(dataloader):\n",
        "    fusion_model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio_x = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            images = batch[\"images\"].to(device)\n",
        "            labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "            audio_emb = audio_model(audio_x)\n",
        "            text_emb = text_model(input_ids, attention_mask)\n",
        "            vision_emb = vision_model(images)\n",
        "\n",
        "            outputs = fusion_model(audio_emb, text_emb, vision_emb).squeeze(1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct += (preds == labels.bool()).sum().item()\n",
        "            total += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "# ------------------ SAVE FUSION MODEL ------------------\n",
        "def save_fusion():\n",
        "    torch.save(fusion_model.state_dict(), \"fusion_model.pt\")\n",
        "\n",
        "# Call save_fusion to create the .pt file\n",
        "save_fusion()"
      ],
      "metadata": {
        "id": "-sfWFqBINNkK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install missing libraries\n",
        "!pip install pydub\n",
        "!pip install SpeechRecognition\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import librosa # For extract_audio_features\n",
        "import speech_recognition as sr # For process_text\n",
        "from pydub import AudioSegment # For process_audio\n",
        "import dlib # For face detection in crop_align_face\n",
        "from transformers import AutoTokenizer, AutoModel, BertModel # For TextModel and tokenize_text\n",
        "from torchvision import models # For VisionModel\n",
        "import shutil # For cleaning up temporary frames\n",
        "\n",
        "\n",
        "# ------------------ CLASS DEFINITIONS (INLINED) ------------------\n",
        "# Audio Model Definition (copied from W-lU2qGHJYXT)\n",
        "class AudioModel(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128, num_layers=2):\n",
        "        super(AudioModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is expected to be (batch_size, seq_len, input_dim)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
        "        return self.fc(h)\n",
        "\n",
        "# TextModel Definition (copied from FADbM2wULsvC)\n",
        "class TextModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.fc = nn.Linear(768, 256)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.fc(cls_embedding)\n",
        "\n",
        "# VisionModel Definition (copied from dP-cJzC4NBN6)\n",
        "class VisionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisionModel, self).__init__()\n",
        "        self.cnn = models.efficientnet_b0(pretrained=True)\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "        self.fc = nn.Linear(1280, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.cnn(x)\n",
        "        return self.fc(features)\n",
        "\n",
        "# MultimodalFusion Definition (copied from 1lUJQMhtNqWX)\n",
        "class MultimodalFusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultimodalFusion, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_emb, text_emb, vision_emb):\n",
        "        fused = torch.cat((audio_emb, text_emb, vision_emb), dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# ------------------ HELPER FUNCTIONS (INLINED AND ADAPTED) ------------------\n",
        "\n",
        "# Initialize dlib components for crop_align_face\n",
        "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "# Function to crop and align face from an image (adapted to return PIL Image)\n",
        "def crop_align_face(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Warning: Could not read image {image_path}. Skipping.\")\n",
        "        return []\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces_detected = detector(gray)\n",
        "\n",
        "    aligned_face_images = []\n",
        "    for face in faces_detected:\n",
        "        landmarks = predictor(gray, face)\n",
        "        left_eye = (landmarks.part(36).x, landmarks.part(36).y)\n",
        "        right_eye = (landmarks.part(45).x, landmarks.part(45).y)\n",
        "\n",
        "        dy = right_eye[1] - left_eye[1]\n",
        "        dx = right_eye[0] - left_eye[0]\n",
        "        angle = np.degrees(np.arctan2(dy, dx))\n",
        "\n",
        "        eyes_center = ((left_eye[0] + right_eye[0]) // 2,\n",
        "                       (left_eye[1] + right_eye[1]) // 2)\n",
        "\n",
        "        M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
        "        aligned_img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR)\n",
        "\n",
        "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "        cropped_face = aligned_img[y:y+h, x:x+w]\n",
        "\n",
        "        cropped_face_rgb = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
        "        aligned_face_images.append(Image.fromarray(cropped_face_rgb))\n",
        "    return aligned_face_images\n",
        "\n",
        "# Extract audio features (adapted to return MFCC tensor)\n",
        "def extract_audio_features(audio_path):\n",
        "  y, sr = librosa.load(audio_path, sr = 16000)\n",
        "  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40) # n_mfcc matches AudioModel's input_dim\n",
        "  # mfccs shape is (n_mfcc, time_frames). LSTM expects (seq_len, input_dim) after batch.\n",
        "  # So transpose to (time_frames, n_mfcc)\n",
        "  return torch.tensor(mfccs.T, dtype=torch.float32)\n",
        "\n",
        "# Initialize tokenizer for tokenize_text\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = embed_tokenizer(text, return_tensors = \"pt\", truncation = True, padding = True)\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "\n",
        "# ------------------ DEVICE ------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------ LOAD PRETRAINED MODELS ------------------\n",
        "audio_model = AudioModel().to(device)\n",
        "text_model = TextModel().to(device)\n",
        "vision_model = VisionModel().to(device)\n",
        "fusion_model = MultimodalFusion().to(device)\n",
        "\n",
        "# Load state dicts\n",
        "# Check if .pt files exist before loading, as they might not be generated if upstream cells failed\n",
        "if os.path.exists(\"audio_model.pt\"):\n",
        "    audio_model.load_state_dict(torch.load(\"audio_model.pt\"))\n",
        "else:\n",
        "    print(\"Warning: 'audio_model.pt' not found. Audio model might not be initialized with trained weights.\")\n",
        "\n",
        "if os.path.exists(\"text_model.pt\"):\n",
        "    text_model.load_state_dict(torch.load(\"text_model.pt\"))\n",
        "else:\n",
        "    print(\"Warning: 'text_model.pt' not found. Text model might not be initialized with trained weights.\")\n",
        "\n",
        "if os.path.exists(\"vision_model.pt\"):\n",
        "    vision_model.load_state_dict(torch.load(\"vision_model.pt\"))\n",
        "else:\n",
        "    print(\"Warning: 'vision_model.pt' not found. Vision model might not be initialized with trained weights.\")\n",
        "\n",
        "# fusion_model.pt is typically saved after training the fusion network\n",
        "# If it's intended to be loaded here, it also needs to be saved after its training loop.\n",
        "# Assuming it might not exist yet, we'll add a similar check.\n",
        "if os.path.exists(\"fusion_model.pt\"):\n",
        "    fusion_model.load_state_dict(torch.load(\"fusion_model.pt\"))\n",
        "else:\n",
        "    print(\"Warning: 'fusion_model.pt' not found. Fusion model might not be initialized with trained weights.\")\n",
        "\n",
        "for model in [audio_model, text_model, vision_model, fusion_model]:\n",
        "    model.eval()\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# ------------------ PREDICTION FUNCTION ------------------\n",
        "def predict(audio_x=None, input_ids=None, attention_mask=None, images=None):\n",
        "    with torch.no_grad():\n",
        "        audio_emb = audio_model(audio_x.to(device).unsqueeze(0)) if audio_x is not None else None # Add batch dim for audio LSTM\n",
        "        text_emb = text_model(input_ids.to(device), attention_mask.to(device)) if input_ids is not None else None\n",
        "        vision_emb = vision_model(images.to(device)) if images is not None else None\n",
        "\n",
        "        # Ensure all embeddings are available for multimodal fusion\n",
        "        if audio_emb is None or text_emb is None or vision_emb is None:\n",
        "            # This case should ideally not happen if all preprocessing steps succeed.\n",
        "            # For a robust system, you might want to handle partial inputs.\n",
        "            raise ValueError(\"All modalities (audio, text, vision) must be provided for prediction.\")\n",
        "\n",
        "        output = fusion_model(audio_emb, text_emb, vision_emb).squeeze(1)\n",
        "        prob = torch.sigmoid(output).cpu().item()\n",
        "    return prob\n",
        "\n",
        "# ------------------ VIDEO PROCESSING ------------------\n",
        "def process_video(video_path, tmp_folder=\"tmp_frames\"):\n",
        "    os.makedirs(tmp_folder, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    frames_paths = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_path = os.path.join(tmp_folder, f\"frame_{frame_count:04d}.jpg\") # Added padding to frame count\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        frames_paths.append(frame_path)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames_paths\n",
        "\n",
        "# ------------------ AUDIO PROCESSING ------------------\n",
        "def process_audio(video_path, audio_output_path=\"temp_audio.wav\"):\n",
        "    # Use moviepy.editor to extract audio\n",
        "    clip = mp.VideoFileClip(video_path)\n",
        "    clip.audio.write_audio_file(audio_output_path, fps=16000)\n",
        "    clip.close()\n",
        "\n",
        "    features = extract_audio_features(audio_output_path)\n",
        "    return features\n",
        "\n",
        "# ------------------ TEXT PROCESSING ------------------\n",
        "def process_text(audio_path):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        audio_data = r.record(source)\n",
        "        text = r.recognize_google(audio_data)\n",
        "    input_ids, attention_mask = tokenize_text(text)\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "# ------------------ VISUAL PROCESSING ------------------\n",
        "def process_frames(frame_paths):\n",
        "    aligned_faces = []\n",
        "    for frame_path in frame_paths:\n",
        "        faces = crop_align_face(frame_path)  # Now returns list of PIL images\n",
        "        aligned_faces.extend(faces)\n",
        "\n",
        "    if not aligned_faces:\n",
        "        print(\"No faces detected in frames.\")\n",
        "        return torch.tensor([])\n",
        "\n",
        "    # Resize and convert to tensor (assuming all images are 224x224 for EfficientNet_b0)\n",
        "    # EfficientNet_b0 expects (batch, channels, height, width)\n",
        "    transformed_images = []\n",
        "    for face_img in aligned_faces:\n",
        "        # Resize to 224x224 as expected by EfficientNet_b0\n",
        "        face_img = face_img.resize((224, 224))\n",
        "        img_array = np.array(face_img).transpose(2,0,1) / 255.0 # (H,W,C) to (C,H,W), normalize\n",
        "        transformed_images.append(img_array)\n",
        "\n",
        "    images_tensor = torch.tensor(transformed_images, dtype=torch.float32)\n",
        "    return images_tensor\n",
        "\n",
        "# ------------------ MAIN PREDICTION FUNCTION ------------------\n",
        "def main(video_file):\n",
        "    tmp_folder = \"tmp_prediction_assets\"\n",
        "    os.makedirs(tmp_folder, exist_ok=True)\n",
        "    audio_output_path = os.path.join(tmp_folder, \"temp_audio.wav\")\n",
        "    frames_output_dir = os.path.join(tmp_folder, \"frames\")\n",
        "\n",
        "    try:\n",
        "        # Process video frames\n",
        "        print(\"Processing video frames...\")\n",
        "        frame_paths = process_video(video_file, tmp_folder=frames_output_dir)\n",
        "        images_tensor = process_frames(frame_paths)\n",
        "        # Average vision embeddings or take first if multiple frames/faces or handle as sequence\n",
        "        if images_tensor.numel() == 0: # Check if tensor is empty\n",
        "             vision_input = None\n",
        "             print(\"Warning: No valid vision input generated.\")\n",
        "        else:\n",
        "            # For simplicity, let's take the first image if multiple frames/faces are processed.\n",
        "            # A more robust approach might involve averaging embeddings or using a sequence model.\n",
        "            vision_input = images_tensor[0:1] # Take first image, keep batch dimension\n",
        "\n",
        "        # Process audio\n",
        "        print(\"Extracting audio...\")\n",
        "        audio_input = process_audio(video_file, audio_output_path)\n",
        "        # AudioModel expects (batch_size, seq_len, input_dim). audio_input is (seq_len, input_dim)\n",
        "        # It will be unsqueezed in predict()\n",
        "\n",
        "        # Process text\n",
        "        print(\"Transcribing audio to text and extracting text features...\")\n",
        "        input_ids, attention_mask = process_text(audio_output_path)\n",
        "        # input_ids and attention_mask are already (batch_size, seq_len)\n",
        "\n",
        "        # Predict\n",
        "        print(\"Making prediction...\")\n",
        "        prob = predict(audio_x=audio_input, input_ids=input_ids, attention_mask=attention_mask, images=vision_input)\n",
        "        print(f\"Deepfake Probability: {prob:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "    finally:\n",
        "        # Clean up temporary files\n",
        "        if os.path.exists(tmp_folder):\n",
        "            shutil.rmtree(tmp_folder)\n",
        "            print(f\"Cleaned up temporary folder: {tmp_folder}\")\n",
        "\n",
        "# ------------------ RUN ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # IMPORTANT: Ensure a valid video file is present in 'Data/videos' for this to work.\n",
        "    # Replace \"dummy_video.mp4\" with the actual path to your video file.\n",
        "    video_file = \"Data/videos/your_video.mp4\"  # <--- REPLACE 'your_video.mp4' with your actual video filename (e.g., 'my_real_video.mp4')\n",
        "\n",
        "    print(\"Please upload a real video file to 'Data/videos' and update 'video_file' variable for actual prediction.\")\n",
        "    print(f\"Currently trying to process: {video_file}\")\n",
        "    main(video_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcejotwLOCJI",
        "outputId": "9630a6e6-ce82-408c-9f63-7b4c164ebdb2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.12/dist-packages (3.14.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from SpeechRecognition) (4.15.0)\n",
            "Please upload a real video file to 'Data/videos' and update 'video_file' variable for actual prediction.\n",
            "Currently trying to process: Data/videos/your_video.mp4\n",
            "Processing video frames...\n",
            "No faces detected in frames.\n",
            "Warning: No valid vision input generated.\n",
            "Extracting audio...\n",
            "An error occurred during prediction: MoviePy error: the file Data/videos/your_video.mp4 could not be found!\n",
            "Please check that you entered the correct path.\n",
            "Cleaned up temporary folder: tmp_prediction_assets\n"
          ]
        }
      ]
    }
  ]
}